\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\def\*#1{\mathbf{#1}}
\def\R{\mathbb{R}}

\begin{document}

\title{Note on a geometric approach to total variation filter}
\author{}

\maketitle

\section{Definitions}

\begin{definition}{(Total Variation)}
  We call {\em total variation} the function $TV:\R^n \rightarrow \R^n$:
  \[TV(\*y) = \sum_{i < n} |y_{i+1} - y_{i}| \]
\end{definition}

\begin{definition}{(Total Variation Filter)}
  Given $\lambda \in \R$, $\lambda \geq 0$ and $\*y \in \R^n$, let $C_{\*y, \lambda}: \R^n \rightarrow \R^n$
  be the function:
    \[ C_{\*y, \lambda} (\*x) = \|\*x - \*y \|^2_2 + TV(\*x) \]

  The problem of minimising $C_{\*y, \lambda}$ is called {\em total varition filter}.
\end{definition}

Since $C_{\*y, \lambda}$ is convex and non-negative it admits a finite unique global minimum,
which will be denoted by $\hat{\*y}$ in the following.

\section{Equivalence of total variation filter and taut string problem}

Let $\*1 \in \R^n$ be the vector of all $1$.

The following fact is an immediate consequence of the definition of $TV$.

\begin{lemma} \label{lemma:TVconst}
For any $\*x \in \R^{n}, \delta \in \mathbb{R}$
  \[ TV(\*x + \delta \mathbf{1}) = TV(\*x) \]
\end{lemma}

\begin{lemma} $\sum_j y_{j} = \sum_j \hat{y}_{j}$ \end{lemma}
\begin{proof}
  For any $\delta \in \R$ let:
  $f(\delta) = \|y - \hat{y}\ + \delta \mathbf{1}\|^2_2 + \lambda TV(\hat{y} + \delta \mathbf{1})$.
  By lemma ~\ref{lemma:TVconst}, $f(\delta) = \|\hat{y}\ + \delta \mathbf{1} - y\|^2_2 + \lambda TV(\hat{y})$.
  Hence $f$ is differentiable and $f'(0) = 2 \sum_j (\hat{y}_j - y_{j})$. But the definition of $\hat {y}$ implies
  that $0$ is a minimum of $f$, so $f'(0) = 0$.
\end{proof}

Let $\*s, \hat{\*s} \in R^{n }$ be the {\em partial sums} of respectively $\*y$ and $\hat{\*y}$
starting from $0$. In other words: $s_i = \sum_{j \leq i} y_{j}$ and $\hat{s}_i = \sum_{j \leq i} \hat{y}_{j}$.

\begin{lemma}\label{bounding}
  For any $j \in [1, n]$:
  \begin{enumerate}
    \item $ \hat{y}_{j} > \hat{y}_{j + 1} \Rightarrow \hat{s}_j = s_j - \lambda / 2$
    \item $ \hat{y}_{j} < \hat{y}_{j + 1} \Rightarrow \hat{s}_j = s_j + \lambda / 2$
    \item $ \hat{y}_{j} = \hat{y}_{j + 1} \Rightarrow |\hat{s}_j - s_j| < \lambda / 2$
  \end{enumerate}
\end{lemma}

\begin{proof}
  For any index $j \in [1, n]$, define $\*u^j \in \R^n$ be such that $u^j_i = 1$ for $i \leq j$ and $\*u^j_i = 0$ for $i > j$ and let $f(\delta) = C^{TVF}_{\*y, \lambda}(\hat{\*y} + \delta \*u^j) - C^{TVF}_{\*y, \lambda}(\hat{\*y})$.
  
  It is useful to decompose $f$ as $f_E + f_{TV}$, where $f_E(\delta) = \|\hat{\*y} + \delta \*u^j - \*y\|^2_2 - \|\hat{\*y} - \*y\|^2_2$ and $f_{TV} (\delta) = \lambda (TV(\hat{\*y} + \delta \*u^j) - TV(\hat{\*y}))$.

  We have $f(0) = 0$ and $f(\delta) > 0$ for $\delta \neq 0$ from the definition of $\hat{\*y}$. 

  To prove 1), assume $\hat{\*y}(j) > \hat{\*y}(j+1)$ for some $j$.

  Observe that, for sufficiently small $\delta$, it is $TV (\hat{\*y} + \delta \*u^j) =
  TV(\hat{\*y}) + \delta$ and $f_{TV} (\delta) = \lambda \delta$. Therefore there exist an open interval $I$ containing $0$ such that $f$ restricted to $I$ can be written as: $f(\delta) = f_E(\delta) + \lambda \delta$ and $f$ is differentiable in $I$. Since $0$ is a minimum for $f$ we must have $f'(0) = f'_{E} + \lambda = 2 (\hat{s}_j - s_j) + \lambda = 0$, wihch means $\hat{s}_j = s_j - \lambda / 2$ and the first point is proved.

  The proof of 2) is entirely symmetric.

  To prove 3) we follow a similar argument. Assume $\hat{y}_j = \hat{y}_{j + 1}$. 

  First of all, observe that in this case: $f_{TV} (\delta) = \lambda |\delta|$.
  Moreover, by simple algebra, $f_E(\delta) = 2\delta (\*u^j, \hat{\*y} - \*y) + \delta^2 \|\*u^j\|^2_2 = 2 (\hat{s}_j - s_j) \delta + C \delta^2$, where we put $C = \|u\|^2_2$ and used $(,)$ to indicate the dot product in $\R^n$.

  Now, let us use the sign function ${\tt sign}$, to write $f(\delta) = |\delta| (\lambda + 2 (\hat{s}_j - s_j){\tt sign} (\delta) + C |\delta|)$, where recall $C > 0$. For this quantity to be positive for arbitrary $\delta > 0$ it must necessarily be $\lambda + 2 (\hat{s}_j  - s_j) > 0 \Rightarrow \hat{s}_j - s_j \geq -\lambda / 2$. Similarly, considering negative $\delta$, it is necessary that $\hat{s}_j - s_j \leq \lambda / 2$. These two inequalities finally prove point 3).    
\end{proof}

We are now able to prove an equivalence between {\em total variation filter} and a seemingly unrelated geometric problem:

\begin{definition}{\em The taut string problem}
  For $\*y \in R^n$ and $\lambda \in \R^n$ with $\delta \geq 0$, let $\*s$ be the partial sum vector of $\*y$. The {\em taut string}
  problem consists in determining an $\*t \in \R^n$ such that:
  \begin{itemize}
    \item $t_j \in [s_j - \lambda/2, s_j + \lambda / 2]$, for $j < n$
    \item $t_n = s_n$
    \item The total length of the chain of segments in $\R^2$ connecting points \[(0, 0), (1, t_1), \dots, (n, t_n)\] is minimum.
  \end{itemize}
\end{definition}

The following is a simple geometric fact characterising the $\*t$ in terms of its local properties:

\begin{lemma}\label{variation}
  A vector $\*t$ satisfying $t_i \in [s_i - \lambda/2, s_i + \lambda/2]$ for each $i$ solves the taut string problem for $\*y$ and $\lambda$ if and only if for all $i \ in [1, n]$:
  \begin{itemize}
    \item $(t_{i-1} + t_{i + 1}) / 2 < t_i \Rightarrow t_i = s_i - \lambda/2$
    \item $(t_{i-1} + t_{i + 1}) / 2 > t_i \Rightarrow t_i = s_i + \lambda/2$
  \end{itemize}
  where $\*s$ is the vector of partial sums of $y$ and we conventionally set $t_0 = 0$
\end{lemma}
\begin{proof} (sketch)
  Since the taut string problem is convex, a solution is the global optimum if and only if it is a local minimum in each coordinates.
  That means that $\*t$ is a solution of the taut string problem if and only if changing anyone of its components increases the
  overall length of the string. In other words, let $f_i(x) = |\overline{t_{i-1}x}| + |\overline{xt_{i+1}}|$, then $\*t$ is the taut string solution
  iff, for every $i$, the minimum of $f_i$ over the admissible interval $[s_i - \lambda/2, s_i + \lambda/2]$ is $t_i$.
  But it is easy to see that $f_i$ is minimised at $m = (t_{i-1} + t_{i+1})/2$ and that $f(x)$ increases as $|x - m|$ increases; as a consequence, $f_i$ having a minimum at $t_i$ over its admissible interval is equivalent to conditions 1, 2.
\end{proof}


\begin{theorem}{(Equivalence of total variation filter and taut string)}
  Let $\hat{\*y}$ be the solution of the total variation filter with parameters $\*y$ and $\lambda$. Then $\hat{\*s}$, the vector of partial sum of $\hat{\*y}$, is the solution of the taut string problem of parameters $\*y$ and $\lambda$
\end{theorem}
\begin{proof}
  Let $\hat{\*y}$ be the total variation filter solution and $\hat{\*s}$ its partial sums vector.
  We will be done if we prove that $\hat{\*s}$ satisfies conditions 1 and 2 of Lemma~\ref{variation}.
  Assume that $\hat{s}_i > (\hat{s}_{i-1} + \hat{s}_{i + 1})/2$. Note that $(\hat{s}_{i-1} + \hat{s}_{i + 1})/2 = \hat{s}_{i-1} + (\hat{y}_i + \hat{y}_{i+1})/2$ therefore we derive $\hat{y}_{i+1} < \hat{y}_i$ and, by Lemma~\ref{bounding}, $\hat{s}_{i} = s_i - \lambda/2$. Symmetric argument prove that $\hat{s}_i > (\hat{s}_{i-1} + \hat{s}_{i + 1})/2$ implies $\hat{s}_{i} = s_i + \lambda/2$
\end{proof}
  
\end{document}
